(1) ğŸ§  Spiral Classification with Neural Networks
This project shows how to classify spiral-shaped data using a neural network built with TensorFlow/Keras.
It tackles a classic non-linear classification problem involving three intertwined spirals! ğŸŒ€ğŸŒ€ğŸŒ€

ğŸ“¦ What's Inside?
âœ… Spiral data generation
âœ… Data preprocessing & scaling ğŸ”„
âœ… Neural network implementation ğŸ¤–
âœ… Model training with smart callbacks âš™ï¸
âœ… Performance evaluation ğŸ“Š
âœ… Cool visualizations ğŸ¨

ğŸ“Š Dataset Details
ğŸ“Œ Type: Synthetic spiral data

ğŸ¯ Classes: 3 (labels: 0, 1, 2)

ğŸ”¢ Points: 999 (333 per class)

âœ‚ï¸ Split: 80% Training / 20% Testing

ğŸ“ Features: 2D coordinates (x, y)

ğŸ‹ï¸ Model Training
âš™ï¸ Optimizer: Adam

ğŸ§® Loss Function: Sparse Categorical Crossentropy

ğŸ“ˆ Metric: Accuracy

ğŸ§  Callbacks:
â¹ï¸ EarlyStopping: patience=20

ğŸ“‰ ReduceLROnPlateau: factor=0.1, patience=10

â³ Max Epochs: 200 (training stops early if no improvement)

ğŸ† Results
ğŸ“Œ Typical Accuracy

âœ… Training: ~99%




(2) # Neural Network from Scratch (Forward Pass)

A minimal implementation of a 2-layer neural network for multi-class classification, demonstrating the forward pass, activation functions, and loss calculation.

## Features
- **Layers**: `Dense` (fully connected), `ReLU`, `Softmax`.
- **Loss**: Categorical cross-entropy.
- **Data**: Spiral dataset (3 classes, 100 samples).

## Code Structure
1. **Data Generation**: `spiral_data()` creates non-linear, separable data.
2. **Layers**:
   - `Layer_Dense`: Linear transformation (`WX + b`).
   - `Activation_ReLU`: Non-linearity via `max(0, x)`.
   - `Activation_Softmax`: Outputs class probabilities.
3. **Loss**: `Loss_CategoricalCrossentropy` computes cross-entropy.
4. **Metrics**: Accuracy via `np.argmax` comparison.

# Create network
dense1 = Layer_Dense(2, 3)      # Input: 2D, Output: 3 neurons
activation1 = Activation_ReLU()
dense2 = Layer_Dense(3, 3)      # Output: 3 classes
activation2 = Activation_Softmax()

# Forward pass
dense1.forward(X)
activation1.forward(dense1.output)
dense2.forward(activation1.output)
activation2.forward(dense2.output)

# Evaluate
loss = loss_function.calculate(activation2.output, y)
accuracy = np.mean(np.argmax(activation2.output, axis=1) == y)

âœ… Validation: ~98%

ğŸ§© Confusion Matrix: Shows clear class separation with excellent performance!




# SPIRAL DATASET RESOLVED BY BACKPROPAGATION (WITHOUT OPTIMIZER)

This is a complete implementation of a 2-layer neural network with forward and backward propagation, designed specifically for classifying spiral data ğŸŒªï¸. It demonstrates core deep learning concepts using only NumPy, without any high-level frameworks.

âš™ï¸ Exact Code Features
ğŸ§± Layers
Layer_Dense (Fully-connected layer):

âœ… Weight initialization: 0.01 * np.random.randn(...)

âœ… Bias initialization: np.zeros(...)

ğŸ”„ Forward & backward propagation supported

âš¡ Activations
Activation_ReLU:

Forward pass âœ”ï¸

Backward pass with gradient clipping ğŸ”

Activation_Softmax:

Stable implementation (max subtraction for numerical stability) ğŸ§®

Combined: Softmax + CrossEntropy for optimized gradients ğŸš€

ğŸ“‰ Loss
Loss_CategoricalCrossentropy:

Accepts raw class labels: [0, 1, 2] ğŸ”¢

Accepts one-hot encoded labels: [[1,0,0], ...] âœ…

Numerical stability with output clipping: [1e-7, 1 - 1e-7] ğŸ›¡ï¸

ğŸ“Š Data
Uses: nnfs.datasets.spiral_data(samples=100, classes=3) ğŸŒªï¸

Visualized via Matplotlib:
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='brg')

The script will
âœ… Generate spiral data
âœ… Run forward propagation
âœ… Compute loss and accuracy
âœ… Perform backpropagation

ğŸ–¨ï¸ Expected Output:

First 5 outputs:
[[0.33 0.33 0.33]
 [0.33 0.33 0.33]
 ...]

loss: 1.0986
acc: 0.34
ğŸ” Key Technical Details

ğŸ” Forward Pass
dense1.forward(X)                     # Input â†’ Hidden (2â†’3)
activation1.forward(dense1.output)   # ReLU Activation
dense2.forward(activation1.output)   # Hidden â†’ Output (3â†’3)
loss = loss_activation.forward(dense2.output, y)  # Softmax + Loss

ğŸ” Backward Pass
loss_activation.backward(loss_activation.output, y)
dense2.backward(loss_activation.dinputs)
activation1.backward(dense2.dinputs)
dense1.backward(activation1.dinputs)
ğŸ“ Gradient Calculations
Weights: inputs.T @ dvalues

Biases: np.sum(dvalues, axis=0)

Inputs: dvalues @ weights.T

ğŸ“ˆ Visualization
Spiral data example:
ğŸŒ€ (Code will generate and plot automatically with Matplotlib)

